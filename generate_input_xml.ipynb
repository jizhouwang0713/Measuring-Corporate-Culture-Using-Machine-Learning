{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/Users/jizhouw0/Desktop/sample_transcripts_xml/20000426-16448-C.xml') as f:\n",
    "    file = f.read()\n",
    "\n",
    "\n",
    "call = BeautifulSoup(file, 'xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<class 'bs4.element.ResultSet'> <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# experiment with the meta section\n",
    "date = call.find('datedf').text if call.find('datedf') != None else \"NA\"\n",
    "transcript_id = call.find('transcript').get('id2')\n",
    "print(transcript_id)\n",
    "\n",
    "company = call.find_all('company')\n",
    "for c in company: print(type(company), type(c.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# experiment with the management discussion section\n",
    "discussion = call.find('section', {'name': 'MANAGEMENT DISCUSSION SECTION'}).find_all('speaker')\n",
    "# Note that the first sectence is an introduction and is not part of the earnings call\n",
    "# Implementation: we wnat to collect all those sentences somewhere -- not important\n",
    "# print(discussion[0].get('id') is None)\n",
    "\n",
    "full_discussion_list = []\n",
    "for part in discussion:\n",
    "    if part.get('id') not in (None, '0'):\n",
    "        paragraphs = part.find_all('p')\n",
    "        full_discussion_list.extend([paragraph.text for paragraph in paragraphs])\n",
    "        \n",
    "full_discussion = \" \".join(full_discussion_list)\n",
    "print(full_discussion)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment with the Q&A section\n",
    "QA = call.find('section', {'name': 'Q&A'}).find_all('speaker')\n",
    "full_QA_list = []\n",
    "for part in QA:\n",
    "    if part.get('id') != \"0\":\n",
    "        paragraphs = part.find_all('p')\n",
    "        full_QA_list.extend([paragraph.text for paragraph in paragraphs])\n",
    "        \n",
    "full_QA = \" \".join(full_QA_list)\n",
    "print(full_QA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define directory where the transcripts are stored \n",
    "directory = \"C:/Users/jizhouw0/Desktop/sample_transcripts_xml\"\n",
    "\n",
    "# define a helper function -- line_counter\n",
    "def line_counter(a_file):\n",
    "    \"\"\"Count the number of lines in a text file\n",
    "    \n",
    "    Arguments:\n",
    "        a_file {str or Path} -- input text file\n",
    "    \n",
    "    Returns:\n",
    "        int -- number of lines in the file\n",
    "    \"\"\"\n",
    "    n_lines = 0\n",
    "    with open(a_file, \"rb\") as f:\n",
    "        n_lines = sum(1 for _ in f)\n",
    "    return n_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a function that is used to extract desired parts from an earnings call transcript\n",
    "def extract_sections(path_str):\n",
    "    # initialize two variables to indicate whether the scripted and unscripted sections exist\n",
    "    no_management_discussion = 0\n",
    "    no_QA = 0\n",
    "    \n",
    "    with open(path_str) as f: \n",
    "            file = f.read()\n",
    "            \n",
    "    call = BeautifulSoup(file, 'xml')\n",
    "    \n",
    "    # First deal with the meta data \n",
    "    transcript_id = call.find('transcript').get('id') if call.find('transcript').get('id') != None else 'NA'\n",
    "    date = call.find('date').text if call.find('date') != None else 'NA'\n",
    "    title = call.find('title').text if call.find('title') != None else 'NA'\n",
    "    company_id = call.find('company').text if call.find('company') != None else 'NA'\n",
    "    \n",
    "    id2firms[\"document_id\"].append(transcript_id)\n",
    "    id2firms[\"firm_id\"].append(company_id)\n",
    "    id2firms[\"time\"].append(date)  \n",
    "    id2firms[\"transcript_title\"].append(title)\n",
    "    \n",
    "    # Then deal with the management discussion section\n",
    "    if call.find('section', {'name': 'MANAGEMENT DISCUSSION SECTION'}) == None:\n",
    "        no_management_discussion = 1\n",
    "        full_discussion = \"\"\n",
    "    else:\n",
    "        discussion = call.find('section', {'name': 'MANAGEMENT DISCUSSION SECTION'}).find_all('speaker')\n",
    "        # Note that we exclude the words said by the moderator (id == 0)\n",
    "        \n",
    "        full_discussion_list = []\n",
    "        for part in discussion:\n",
    "            if part.get('id') not in (None, '0'):\n",
    "                paragraphs = part.find_all('p')\n",
    "                full_discussion_list.extend([paragraph.text for paragraph in paragraphs])\n",
    "                \n",
    "        full_discussion_raw = \" \".join(full_discussion_list).replace('\\r', '').replace('\\n', '')\n",
    "        # remove any extra space\n",
    "        full_discussion = \" \".join(full_discussion_raw.split())\n",
    "        \n",
    "    # Finally deal with the Q & A section\n",
    "    if  call.find('section', {'name': 'Q&A'}) == None:\n",
    "        no_QA = 1\n",
    "        full_QA = \"\"\n",
    "    else:\n",
    "        QA = call.find('section', {'name': 'Q&A'}).find_all('speaker')\n",
    "        full_QA_list = []\n",
    "        for part in QA:\n",
    "            if part.get('id') != \"0\":\n",
    "                paragraphs = part.find_all('p')\n",
    "                full_QA_list.extend([paragraph.text for paragraph in paragraphs])\n",
    "                \n",
    "        full_QA_raw = \" \".join(full_QA_list).replace('\\r', '').replace('\\n', '')\n",
    "        # remove any extra space\n",
    "        full_QA = \" \".join(full_QA_raw.split())\n",
    "        \n",
    "    # Combine management discussion with Q&A to form the complete transcripts\n",
    "    full_transcript = full_discussion + \" \" + full_QA\n",
    "    \n",
    "    # Output management discussion and Q&A to text files \n",
    "    with open('C:/Users/jizhouw0/Desktop/sample_transcripts_xml/text_all/transcripts_both.txt', 'a', encoding=\"utf-8\") as out_f: \n",
    "        out_f.write(full_transcript + '\\n')\n",
    "        \n",
    "    \n",
    "    with open('C:/Users/jizhouw0/Desktop/sample_transcripts_xml/text_all/transcripts_unscripted.txt', 'a', encoding=\"utf-8\") as out_f: \n",
    "        out_f.write(full_QA + '\\n')\n",
    "        \n",
    "        \n",
    "    with open('C:/Users/jizhouw0/Desktop/sample_transcripts_xml/text_all/transcripts_scripted.txt', 'a', encoding=\"utf-8\") as out_f: \n",
    "        out_f.write(full_discussion + '\\n')\n",
    "    \n",
    "    # Return whether the scripted and unscripted sections exist\n",
    "    return no_management_discussion, no_QA\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We iteratre over all the transripts (in xml format) and generate three text files as inout for the ML algorithm \n",
    "# scripted section, Q & A section, and both\n",
    "existence_dict = {}\n",
    "path_dict = {}\n",
    "id2firms = {\"document_id\":[], \"firm_id\": [], \"time\": [], \"transcript_title\": []}\n",
    "doc_count_C, doc_count_T, doc_count_total, doc_count_not_included = 0, 0, 0, 0\n",
    "problematic_file_id = 0\n",
    "\n",
    "# When we loop over the files for the first time, we only focus on those \"corrected\" transcripts\n",
    "for filename in os.scandir(directory):\n",
    "    if filename.is_file():\n",
    "        doc_count_total += 1\n",
    "        if filename.path.find('-C') != -1: \n",
    "            doc_count_C += 1\n",
    "            new_path = filename.path.replace('-C', \"\")\n",
    "            path_dict[new_path] = doc_count_C\n",
    "            \n",
    "            no_management_discussion, no_QA = extract_sections(filename.path)\n",
    "            if no_management_discussion + no_QA > 0:\n",
    "                problematic_file_id += 1\n",
    "                data_row = [0, 0, filename.path.replace('\\\\', '/')]\n",
    "                if no_management_discussion == 1:\n",
    "                    data_row[0] = 1\n",
    "                if no_QA == 1:\n",
    "                    data_row[1] = 1\n",
    "                \n",
    "                existence_dict[problematic_file_id] = data_row\n",
    "                \n",
    "# Now we loop over the files for the second time. We focus on the raw transcripts.\n",
    "for filename in os.scandir(directory):\n",
    "    if filename.is_file(): \n",
    "        if filename.path.find('-T') != -1: \n",
    "            if filename.path.replace('-T', \"\") not in path_dict:\n",
    "                doc_count_T += 1\n",
    "                \n",
    "                no_management_discussion, no_QA = extract_sections(filename.path)\n",
    "                if no_management_discussion + no_QA > 0:\n",
    "                    problematic_file_id += 1\n",
    "                    data_row = [0, 0, filename.path.replace('\\\\', '/')]\n",
    "                    if no_management_discussion == 1:\n",
    "                        data_row[0] = 1\n",
    "                    if no_QA == 1:\n",
    "                        data_row[1] = 1\n",
    "                    \n",
    "                    existence_dict[problematic_file_id] = data_row\n",
    "                    \n",
    "            else:\n",
    "                doc_count_not_included += 1\n",
    "                continue\n",
    "            \n",
    "            \n",
    "# Output the text file \"document_id\"     \n",
    "document_id = map(str, id2firms[\"document_id\"]) \n",
    "with open('C:/Users/jizhouw0/Desktop/sample_transcripts_xml/text_all/document_ids.txt', 'w') as f: \n",
    "    f.write('\\n'.join(document_id))\n",
    "    \n",
    "# Output the csv file \"id2firms\"\n",
    "df_id2firms = pd.DataFrame(data=id2firms)\n",
    "df_id2firms[[\"document_id\", \"firm_id\"]] = df_id2firms[[\"document_id\", \"firm_id\"]].astype(str)\n",
    "df_id2firms.to_csv('C:/Users/jizhouw0/Desktop/sample_transcripts_xml/text_all/id2firms.csv', header = True, index = False)\n",
    "\n",
    "# Output the text file \"meta_data\"\n",
    "with open('C:/Users/jizhouw0/Desktop/sample_transcripts_xml/text_all/meta_data.txt', 'a', encoding=\"utf-8\") as f: \n",
    "    f.write(f'There are a total of {doc_count_total} transcripts in the directory' + '\\n')\n",
    "    f.write(f'The final text file includes {doc_count_C} corrected transcipts' + '\\n')\n",
    "    f.write(f'The final text file includes {doc_count_T} raw transcripts' + '\\n')\n",
    "    f.write(f'{doc_count_not_included} raw transcripts also have the corrected versions, so the raw versions are not included')\n",
    "    \n",
    "# Output a summary of files lacking either the scripted or the unscripted section\n",
    "df_missing_sections = pd.DataFrame.from_dict(existence_dict, orient='index', columns=['no_management_discussion', 'no_Q&A', 'file_path'])\n",
    "df_missing_sections.to_csv('C:/Users/jizhouw0/Desktop/sample_transcripts_xml/text_all/missing_sections.csv', header = True, index = False)\n",
    "\n",
    "# Assert that line numbers equal\n",
    "assert line_counter(\"C:/Users/jizhouw0/Desktop/sample_transcripts_xml/text_all/transcripts_unscripted.txt\") == doc_count_C + doc_count_T, \"Line Number Differs From Document Count!\"\n",
    "assert df_id2firms.shape[0] == line_counter(\"C:/Users/jizhouw0/Desktop/sample_transcripts_xml/text_all/transcripts_both.txt\"), \"Number of IDs Differs From Number of Documents!\"\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "533069\n"
     ]
    }
   ],
   "source": [
    "print(line_counter(\"Z:/Measuring-Corporate-Culture-Using-Machine-Learning/data/input/documents.txt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "352bbe8eef13dbc790a4a10aef27629e00f277446254fc1e3e762725b62d7e10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
